{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eef91d9",
   "metadata": {},
   "source": [
    "# 自然言語処理とは\n",
    "私たちが普段使用している言語は**自然言語**と言い、**自然言語処理**はそれをコンピューターに理解させるための技術分野である。自然言語はコンピューターの理解できるプログラミング言語などと違い、柔軟に意味や形を変える。この差が自然言語処理の難しさを生む。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f30446",
   "metadata": {},
   "source": [
    "## 単語の意味\n",
    "文章を理解させるためにはまず単語の意味を理解させる必要がある。単語の意味を理解させる方法として、以下の3つの手法をここでは紹介する。\n",
    "\n",
    "1. シソーラスによる手法\n",
    "1. カウントベースの手法\n",
    "1. 推論ベースの手法(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76fd38",
   "metadata": {},
   "source": [
    "# シソーラス\n",
    "シソーラスは基本的には類語辞書であり、同義語や類義語が同じグループに分類されている。また、下図のように、単語同士の上位下位の関連性も定義されていることがある。\n",
    "<img src='https://assets.st-note.com/production/uploads/images/66671667/picture_pc_77da28808628a98829596b0cc92c44c6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e24b0",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "WordNetは自然言語処理の分野において最も有名なシソーラスであり、様々な自然言語処理アプリケーションで活躍している。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85628e",
   "metadata": {},
   "source": [
    "## シソーラスの問題点\n",
    "シソーラスには、大きく分けて3つの問題点がある。\n",
    "\n",
    "1. 時代の変化に対応することが困難\n",
    "    - 時代による単語の増加や意味の変化に対応するには更新し続けるしかない\n",
    "1. 人の作業コストが高い\n",
    "    - 人力で単語の関連付けを行う必要がある。\n",
    "1. 単語の細かいニュアンスを表現できない\n",
    "    - 似たような単語でも意味が違う単語をくみ取ることができない ex)ヴィンテージとレトロ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb872d96",
   "metadata": {},
   "source": [
    "# カウントベースの手法\n",
    "この手法では、研究のために収集された大量のテキストデータ（**コーパス**と呼ばれる）を使用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd914fb0",
   "metadata": {},
   "source": [
    "## Pythonによるコーパスの下準備\n",
    "コーパスを利用できる形に変換する実装が以下である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9d90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {} #単語からIDへ変換\n",
    "    id_to_word = {} #IDから単語に変換\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb848eec",
   "metadata": {},
   "source": [
    "## 単語の分散表現\n",
    "コンピューターの世界では、色はRGBを数値化したベクトル表現で表される。単語でも同様にベクトル表現を行うことで定量化し、扱いを容易にすることができる。このような表現を、単語の**分散表現**という。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9d8a1",
   "metadata": {},
   "source": [
    "## 分布仮説\n",
    "「単語の意味は、周囲の単語によって形成される」という仮説を**分布仮説**といい、現代の分散表現の研究はこの仮説に基づいて行われている。これは、コンテキストによって意味が形成されるということである。周囲の単語をどれだけコンテキストの範囲に含めるかを、ウィンドウサイズという言葉で表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f02702",
   "metadata": {},
   "source": [
    "## 共起行列\n",
    "分布仮説に基づいて分散表現を行うとき、単純な方法として、その分散表現の対象となる単語の周囲にどのような単語がどれだけ存在するかをカウントし集計する手法がある。これを「カウントベースの手法」または「統計的手法」という。また、各単語についてそのコンテキストに含まれる単語の頻度をカウントし、テーブルにまとめたものを**共起行列**という。コーパスから共起行列を生成する実装は以下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9672bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''共起行列の作成\n",
    "\n",
    "    :param corpus: コーパス（単語IDのリスト）\n",
    "    :param vocab_size:語彙数\n",
    "    :param window_size:ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
    "    :return: 共起行列\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605bf5f",
   "metadata": {},
   "source": [
    "## ベクトル間の類似度\n",
    "単語のベクトル表現の類似度の計測には、**コサイン類似度**がよく用いられる。コサイン類似度は、$x$と$y$の2つのベクトルがあるとき、次の式で定義される。\n",
    "<br>\n",
    "$$\\displaystyle\n",
    "\\text{similarity}(x,y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} = \\frac{x_1 y_1 + \\cdots + x_n y_n}{\\sqrt{x_1^2 + \\cdots + x_n^2} \\sqrt{y_1^2 + \\cdots + y_n^2}}\n",
    "$$\n",
    "<br>\n",
    "これは、ベクトルがどれほど同じ向きを向いているかを表す。この実装は下記。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df47e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''コサイン類似度の算出\n",
    "\n",
    "    :param x: ベクトル\n",
    "    :param y: ベクトル\n",
    "    :param eps: ”0除算”防止のための微小値\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1886a",
   "metadata": {},
   "source": [
    "今までに実装した関数を用いて、\"You say goodbye and I say hello.\"という文の中での\"you\"と\"I\"の類似度を求めてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e35598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]  #「you」の単語ベクトル\n",
    "c1 = C[word_to_id['i']]  #「i」の単語ベクトル\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43658678",
   "metadata": {},
   "source": [
    "コサイン類似度は1～-1の値をとるため、これは比較的高い数値といえる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05098862",
   "metadata": {},
   "source": [
    "## 類似単語のランキング表示\n",
    "使用しやすいように、ある単語（クエリ）が与えられたとき、その単語に類似した単語を上位から順に返す関数を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7351f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''類似単語の検索\n",
    "\n",
    "    :param query: クエリ（テキスト）\n",
    "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
    "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
    "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
    "    :param top: 上位何位まで表示するか\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7129a",
   "metadata": {},
   "source": [
    "この関数を先ほどのコーパスにおいてyouで適用すると以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8273e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588a1ba",
   "metadata": {},
   "source": [
    "# カウントベースの手法の改善"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039839c",
   "metadata": {},
   "source": [
    "## 相互情報量\n",
    "共起行列では2つの単語が共起した回数を要素として持ったが、単語によって登場頻度は異なるため、頻繁に登場する単語ほど実際には関連性がなくても強い関連を示してしまう。この問題を解決するために、**相互情報量**（PMI）と呼ばれる指標が用いられる。これは、確率変数$x,y$を用いて以下の式で定義される。\n",
    "$${\\displaystyle\n",
    "\\text{PMI}(x,y) = log_2 \\frac{P(x,y)}{P(x)P(y)}\n",
    "}$$\n",
    "<br>\n",
    "PMIは、値が高いほど$x$の発生と$y$の発生の関連性が高いことを示す。また、2つの単語が共起する回数が0回のとき、PMIは-∞をとってしまうため、実践上では**正の相互情報量**（PPMI）を使用する。\n",
    "$${\\displaystyle\n",
    "\\text{PPMI}(x,y) = \\text{max}(0,\\text{PMI}(x,y))\n",
    "}$$\n",
    "<br>\n",
    "共起行列をPPMI行列へ変換する実装は以下である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb096700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI（正の相互情報量）の作成\n",
    "\n",
    "    :param C: 共起行列\n",
    "    :param verbose: 進行状況を出力するかどうか\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100 + 1) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d83cc",
   "metadata": {},
   "source": [
    "これを実際に使用してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09513e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce29a82",
   "metadata": {},
   "source": [
    "変換ができた。しかし、このPPMI行列は、語彙数が増えるにしたがって次元数も増えていく。数万レベルまで語彙数が増えると実用的ではなくなってくる。この問題を解決するために行われるのが次元削減だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c61b08",
   "metadata": {},
   "source": [
    "## 次元削減\n",
    "次元削減は文字通りベクトルの次元を削減する手法である。次元削減にもいくつかの方法があるが、ここでは**特異値分解**（SVD）を用いる。SVDは、任意の行列を3つの行列の積へと分解する。数式では次のように表される。\n",
    "$$\n",
    "X = USV^\\top\n",
    "$$\n",
    "SVDは、任意の行列$X$を、3つの行列$U,S,V$の積に分解する。このとき、$U$と$V$は直交行列であり、$S$は対角行列である。先ほどのコーパスにおけるSVDによる次元削減をPythonで実装したのが下記である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846fe129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[ 3.409e-01 -1.110e-16 -4.441e-16 -1.205e-01  0.000e+00 -9.323e-01\n",
      " -1.086e-16]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ext = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print(C[0])\n",
    "print(W[0])\n",
    "print(U[0])\n",
    "\n",
    "# plot\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754fbb5",
   "metadata": {},
   "source": [
    "ここでは小さなコーパスを用いているため結果がわかりにくい。そのため、より大きなコーパスであるPTBデータデットを用いてみる。ここではPTBコーパスの読み込みに、「ゼロから作るDeepLearning②」で提供されているスクリプトを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dded30d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "corpus size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('corpus size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed169f5",
   "metadata": {},
   "source": [
    "扱うのがより大きな行列となるため、より高速なsklearnモジュールのSVDを利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8fa588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting  co-occurrence ...\n",
      "calculating PPMI ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaito\\AppData\\Local\\Temp\\ipykernel_19224\\3766924727.py:16: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "C:\\Users\\Kaito\\AppData\\Local\\Temp\\ipykernel_19224\\3766924727.py:16: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.7016294002532959\n",
      " we: 0.6388039588928223\n",
      " anybody: 0.5868049263954163\n",
      " do: 0.5612815022468567\n",
      " 'll: 0.5126120448112488\n",
      "\n",
      "[query] year\n",
      " month: 0.6957005262374878\n",
      " quarter: 0.691483736038208\n",
      " earlier: 0.6661214232444763\n",
      " last: 0.6327787041664124\n",
      " third: 0.6230476498603821\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6767407059669495\n",
      " auto: 0.6339930295944214\n",
      " vehicle: 0.597271203994751\n",
      " cars: 0.5888376235961914\n",
      " truck: 0.5693157911300659\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7481387257575989\n",
      " nissan: 0.7147319912910461\n",
      " motors: 0.6946367025375366\n",
      " lexus: 0.6553674936294556\n",
      " honda: 0.6343469023704529\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting  co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5d83f",
   "metadata": {},
   "source": [
    "人間の目で見ても関連性の高いワードが検出されていることが分かる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
